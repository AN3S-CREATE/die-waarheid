# üïµÔ∏è DIE WAARHEID: COMPLETE APPLICATION - PRODUCTION-READY CODE

This is a full, detailed, production-grade implementation with every component from the manual.

---

üì¶ PART 1: PROJECT STRUCTURE \& SETUP

File Structure:

die\_waarheid/

‚îú‚îÄ‚îÄ requirements.txt

‚îú‚îÄ‚îÄ .env                          # API keys (DO NOT COMMIT)

‚îú‚îÄ‚îÄ .gitignore

‚îú‚îÄ‚îÄ README.md

‚îú‚îÄ‚îÄ credentials/

‚îÇ   ‚îú‚îÄ‚îÄ google\_credentials.json   # OAuth client secret

‚îÇ   ‚îî‚îÄ‚îÄ token.pickle              # Auto-generated

‚îú‚îÄ‚îÄ data/

‚îÇ   ‚îú‚îÄ‚îÄ audio/                    # Downloaded voice notes

‚îÇ   ‚îú‚îÄ‚îÄ text/                     # Chat exports

‚îÇ   ‚îú‚îÄ‚îÄ temp/                     # Processing temp files

‚îÇ   ‚îî‚îÄ‚îÄ output/                   # Results

‚îÇ       ‚îú‚îÄ‚îÄ mobitables/

‚îÇ       ‚îú‚îÄ‚îÄ reports/

‚îÇ       ‚îî‚îÄ‚îÄ exports/

‚îú‚îÄ‚îÄ src/

‚îÇ   ‚îú‚îÄ‚îÄ \_\_init\_\_.py

‚îÇ   ‚îú‚îÄ‚îÄ gdrive\_handler.py         # Google Drive integration

‚îÇ   ‚îú‚îÄ‚îÄ forensics.py              # Audio analysis engine

‚îÇ   ‚îú‚îÄ‚îÄ chat\_parser.py            # WhatsApp parser

‚îÇ   ‚îú‚îÄ‚îÄ ai\_analyzer.py            # Gemini AI profiling

‚îÇ   ‚îú‚îÄ‚îÄ mobitab\_builder.py        # Timeline generator

‚îÇ   ‚îî‚îÄ‚îÄ visualizations.py         # Dashboard components

‚îú‚îÄ‚îÄ app.py                        # Main Streamlit application

‚îî‚îÄ‚îÄ config.py                     # Configuration settings

---

üìù PART 2: REQUIREMENTS.TXT (Complete Dependencies)

\# requirements.txt

\# Core Framework

streamlit==1.31.0

python-dotenv==1.0.0

\# Data Processing

pandas==2.1.4

numpy==1.24.3

python-dateutil==2.8.2

pytz==2023.3

regex==2023.12.25

\# Audio Processing

librosa==0.10.1

soundfile==0.12.1

pydub==0.25.1

audioread==3.0.1

\# Machine Learning \& AI

torch==2.1.2

torchaudio==2.1.2

openai-whisper==20231117

pyannote.audio==3.1.1

google-generativeai==0.3.2

textblob==0.17.1

\# Google Drive Integration

google-auth==2.26.2

google-auth-oauthlib==1.2.0

google-auth-httplib2==0.2.0

google-api-python-client==2.111.0

\# Data Visualization

plotly==5.18.0

matplotlib==3.8.2

seaborn==0.13.1

kaleido==0.2.1  # For static image export

\# Utilities

tqdm==4.66.1

pillow==10.1.0

openpyxl==3.1.2  # Excel export support

\# NLP

nltk==3.8.1

spacy==3.7.2

\# Optional Performance Boost

\# numba==0.58.1  # Uncomment for faster librosa processing

---

üîß PART 3: CONFIGURATION FILES

config.py

"""

Configuration Settings for Die Waarheid

Central configuration management

"""

import os

from pathlib import Path

from dotenv import load\_dotenv

\# Load environment variables

load\_dotenv()

\# ==============================================================================

\# DIRECTORY PATHS

\# ==============================================================================

BASE\_DIR = Path(\_\_file\_\_).parent

DATA\_DIR = BASE\_DIR / "data"

AUDIO\_DIR = DATA\_DIR / "audio"

TEXT\_DIR = DATA\_DIR / "text"

TEMP\_DIR = DATA\_DIR / "temp"

OUTPUT\_DIR = DATA\_DIR / "output"

MOBITAB\_DIR = OUTPUT\_DIR / "mobitables"

REPORTS\_DIR = OUTPUT\_DIR / "reports"

EXPORTS\_DIR = OUTPUT\_DIR / "exports"

CREDENTIALS\_DIR = BASE\_DIR / "credentials"

\# Create directories if they don't exist

for directory in \[DATA\_DIR, AUDIO\_DIR, TEXT\_DIR, TEMP\_DIR, OUTPUT\_DIR,

&nbsp;                 MOBITAB\_DIR, REPORTS\_DIR, EXPORTS\_DIR, CREDENTIALS\_DIR]:

&nbsp;   directory.mkdir(parents=True, exist\_ok=True)

\# ==============================================================================

\# API CREDENTIALS

\# ==============================================================================

GEMINI\_API\_KEY = os.getenv("GEMINI\_API\_KEY", "")

HUGGINGFACE\_TOKEN = os.getenv("HUGGINGFACE\_TOKEN", "")

GOOGLE\_CREDENTIALS\_PATH = CREDENTIALS\_DIR / "google\_credentials.json"

TOKEN\_PICKLE\_PATH = CREDENTIALS\_DIR / "token.pickle"

\# ==============================================================================

\# GOOGLE DRIVE SETTINGS

\# ==============================================================================

GDRIVE\_AUDIO\_FOLDER = "Investigation\_Audio"

GDRIVE\_TEXT\_FOLDER = "Investigation\_Text"

GDRIVE\_SCOPES = \['<https://www.googleapis.com/auth/drive.readonly>']

\# ==============================================================================

\# AUDIO PROCESSING SETTINGS

\# ==============================================================================

SUPPORTED\_AUDIO\_FORMATS = \['.opus', '.ogg', '.mp3', '.wav', '.m4a', '.aac']

TARGET\_SAMPLE\_RATE = 16000  # Hz

WHISPER\_MODEL\_SIZE = "medium"  # Options: tiny, base, small, medium, large

\# Whisper Configuration for Afrikaans

WHISPER\_OPTIONS = {

&nbsp;   "language": "af",

&nbsp;   "task": "transcribe",

&nbsp;   "best\_of": 5,

&nbsp;   "beam\_size": 5,

&nbsp;   "patience": 1.0,

&nbsp;   "length\_penalty": 1.0,

&nbsp;   "temperature": 0.0,

&nbsp;   "compression\_ratio\_threshold": 2.4,

&nbsp;   "logprob\_threshold": -1.0,

&nbsp;   "no\_speech\_threshold": 0.6,

}

\# ==============================================================================

\# FORENSIC ANALYSIS THRESHOLDS

\# ==============================================================================

\# Bio-Signal Detection Thresholds

STRESS\_THRESHOLD\_HIGH = 50      # Pitch volatility > 50 = high stress

SILENCE\_RATIO\_THRESHOLD = 0.4   # Silence > 40% = cognitive load

INTENSITY\_SPIKE\_THRESHOLD = 0.7 # RMS energy spike indicator

\# Toxicity Detection

GASLIGHTING\_PHRASES = \[

&nbsp;   "never said that", "you're crazy", "imagining things",

&nbsp;   "overreacting", "too sensitive", "making things up",

&nbsp;   "didn't happen", "in your head", "being dramatic",

&nbsp;   "jy dink te veel", "jy is mal", "dit het nooit gebeur nie"

]

TOXICITY\_PHRASES = \[

&nbsp;   "stupid", "hate", "idiot", "shut up", "dumb", "useless",

&nbsp;   "pathetic", "loser", "worthless", "disgusting",

&nbsp;   "dom", "haat", "idioot", "hou jou bek", "nutteloos"

]

NARCISSISTIC\_PATTERNS = \[

&nbsp;   "I'm the victim", "you made me", "look what you made me do",

&nbsp;   "everyone agrees with me", "nobody likes you",

&nbsp;   "ek is die slagoffer", "jy het my gemaak"

]

\# ==============================================================================

\# MOBITAB SETTINGS

\# ==============================================================================

MOBITAB\_COLUMNS = \[

&nbsp;   "Index",

&nbsp;   "Msg\_ID",

&nbsp;   "Sender",

&nbsp;   "Recorded\_At",

&nbsp;   "Speaker\_Count",

&nbsp;   "Transcript",

&nbsp;   "Tone\_Emotion",

&nbsp;   "Pitch\_Volatility",

&nbsp;   "Silence\_Ratio",

&nbsp;   "Intensity\_Max",

&nbsp;   "Forensic\_Flag"

]

MOBITAB\_FILENAME = "timeline.md"

\# ==============================================================================

\# AI ANALYSIS SETTINGS

\# ==============================================================================

GEMINI\_MODEL = "gemini-1.5-pro"

GEMINI\_TEMPERATURE = 0.2

GEMINI\_MAX\_TOKENS = 8192

\# Gemini Safety Settings (adjust based on content sensitivity)

GEMINI\_SAFETY\_SETTINGS = \[

&nbsp;   {"category": "HARM\_CATEGORY\_HARASSMENT", "threshold": "BLOCK\_NONE"},

&nbsp;   {"category": "HARM\_CATEGORY\_HATE\_SPEECH", "threshold": "BLOCK\_NONE"},

&nbsp;   {"category": "HARM\_CATEGORY\_SEXUALLY\_EXPLICIT", "threshold": "BLOCK\_NONE"},

&nbsp;   {"category": "HARM\_CATEGORY\_DANGEROUS\_CONTENT", "threshold": "BLOCK\_NONE"},

]

\# ==============================================================================

\# VISUALIZATION SETTINGS

\# ==============================================================================

PLOTLY\_THEME = "plotly\_dark"

STRESS\_HEATMAP\_COLORSCALE = "RdYlGn\_r"  # Red (high stress) to Green (calm)

\# Color mapping for speakers

SPEAKER\_COLORS = {

&nbsp;   "SPEAKER\_00": "#1f77b4",  # Blue

&nbsp;   "SPEAKER\_01": "#ff7f0e",  # Orange

&nbsp;   "SPEAKER\_02": "#2ca02c",  # Green

&nbsp;   "SPEAKER\_03": "#d62728",  # Red

&nbsp;   "SPEAKER\_04": "#9467bd",  # Purple

}

\# ==============================================================================

\# BATCH PROCESSING SETTINGS

\# ==============================================================================

BATCH\_SIZE = 20  # Process 20 audio files at a time

MAX\_WORKERS = 4  # Parallel processing threads

\# ==============================================================================

\# PRIVACY \& SECURITY

\# ==============================================================================

ANONYMIZE\_PHONE\_NUMBERS = True

ANONYMIZE\_NAMES = True

AUTO\_CLEANUP\_TEMP\_FILES = True

CLEANUP\_DELAY\_HOURS = 24  # Delete temp files after 24 hours

\# ==============================================================================

\# LOGGING

\# ==============================================================================

LOG\_LEVEL = "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL

LOG\_FILE = BASE\_DIR / "die\_waarheid.log"

\# ==============================================================================

\# REPORT GENERATION

\# ==============================================================================

REPORT\_TEMPLATE = """

\# üïµÔ∏è DIE WAARHEID FORENSIC ANALYSIS REPORT

\*\*Generated:\*\* {timestamp}

\*\*Case ID:\*\* {case\_id}

\*\*Analysis Period:\*\* {date\_range}

---

\## üìä EXECUTIVE SUMMARY

\*\*Total Messages Analyzed:\*\* {total\_messages}

\*\*Voice Notes Analyzed:\*\* {total\_audio}

\*\*Participants:\*\* {participants}

\*\*Analysis Duration:\*\* {duration}

\### Trust Score: {trust\_score}/100

{trust\_interpretation}

---

\## üö® CRITICAL FINDINGS

{critical\_findings}

---

\## üìà COMMUNICATION METRICS

{metrics\_summary}

---

\## üé≠ PSYCHOLOGICAL PROFILE

{psychological\_profile}

---

\## ‚ö†Ô∏è DETECTED CONTRADICTIONS

{contradictions}

---

\## üî¨ BIO-SIGNAL ANALYSIS

{biosignal\_summary}

---

\## üìù RECOMMENDATIONS

{recommendations}

---

\## ‚öñÔ∏è LEGAL DISCLAIMER

This report is generated by AI-assisted analysis and should not be considered

as sole evidence in legal proceedings. All findings should be verified by

qualified professionals. The accuracy of voice stress analysis is subject

to debate in the forensic community.

\*\*Report ID:\*\* {report\_id}

"""

\# ==============================================================================

\# EXPORT SETTINGS

\# ==============================================================================

EXPORT\_FORMATS = \["markdown", "pdf", "html", "json", "csv"]

DEFAULT\_EXPORT\_FORMAT = "markdown"

\# ==============================================================================

\# VALIDATION

\# ==============================================================================

def validate\_config():

&nbsp;   """Validate critical configuration settings"""

&nbsp;   errors = \[]

&nbsp;

&nbsp;   if not GEMINI\_API\_KEY:

&nbsp;       errors.append("‚ùå GEMINI\_API\_KEY not set in .env file")

&nbsp;

&nbsp;   if not HUGGINGFACE\_TOKEN:

&nbsp;       errors.append("‚ö†Ô∏è HUGGINGFACE\_TOKEN not set (speaker diarization will be disabled)")

&nbsp;

&nbsp;   if not GOOGLE\_CREDENTIALS\_PATH.exists():

&nbsp;       errors.append("‚ö†Ô∏è Google Drive credentials not found (manual upload only)")

&nbsp;

&nbsp;   return errors

\# ==============================================================================

\# CONSTANTS

\# ==============================================================================

APP\_NAME = "Die Waarheid"

APP\_VERSION = "1.0.0"

APP\_DESCRIPTION = "Forensic-Grade WhatsApp Communication Analysis Platform"

AUTHOR = "AN3S Workspace"

---

üîê PART 4: ENVIRONMENT FILE (.env)

\# .env - KEEP THIS FILE SECRET! Add to .gitignore

\# Google Gemini API Key

\# Get from: <https://makersuite.google.com/app/apikey>

GEMINI\_API\_KEY=your\_gemini\_api\_key\_here

\# HuggingFace Token

\# Get from: <https://huggingface.co/settings/tokens>

HUGGINGFACE\_TOKEN=your\_huggingface\_token\_here

\# Optional: Custom settings

WHISPER\_MODEL\_SIZE=medium

LOG\_LEVEL=INFO

---

üö´ PART 5: .gitignore

\# Python

\_\_pycache\_\_/

\*.py\[cod]

\*$py.class

\*.so

.Python

env/

venv/

ENV/

build/

develop-eggs/

dist/

downloads/

eggs/

.eggs/

lib/

lib64/

parts/

sdist/

var/

wheels/

\*.egg-info/

.installed.cfg

\*.egg

\# Environment

.env

.venv

\# Credentials \& API Keys

credentials/

\*.json

\*.pickle

token.pickle

\# Data Files

data/audio/\*

data/text/\*

data/temp/\*

data/output/\*

!data/audio/.gitkeep

!data/text/.gitkeep

!data/temp/.gitkeep

!data/output/.gitkeep

\# Logs

\*.log

logs/

\# IDE

.vscode/

.idea/

\*.swp

\*.swo

\*~

\# OS

.DS\_Store

Thumbs.db

\# Model Cache

.cache/

models/

\# Jupyter

.ipynb\_checkpoints/

\# Streamlit

.streamlit/secrets.toml

---

üì° PART 6: GOOGLE DRIVE HANDLER (gdrive\_handler.py)

"""

Google Drive Integration for Die Waarheid

Handles OAuth authentication and file operations

"""

import os

import io

import pickle

from pathlib import Path

from typing import List, Dict, Tuple, Optional

from google.auth.transport.requests import Request

from google.oauth2.credentials import Credentials

from google\_auth\_oauthlib.flow import InstalledAppFlow

from googleapiclient.discovery import build

from googleapiclient.http import MediaIoBaseDownload

from googleapiclient.errors import HttpError

import streamlit as st

from config import (

&nbsp;   GDRIVE\_SCOPES,

&nbsp;   GOOGLE\_CREDENTIALS\_PATH,

&nbsp;   TOKEN\_PICKLE\_PATH,

&nbsp;   GDRIVE\_AUDIO\_FOLDER,

&nbsp;   GDRIVE\_TEXT\_FOLDER,

&nbsp;   AUDIO\_DIR,

&nbsp;   TEXT\_DIR,

&nbsp;   SUPPORTED\_AUDIO\_FORMATS

)

class GDriveHandler:

&nbsp;   """

&nbsp;   Manages Google Drive authentication and file operations

&nbsp;   Implements OAuth 2.0 flow with token persistence

&nbsp;   """

&nbsp;

&nbsp;   def \_\_init\_\_(self):

&nbsp;       self.credentials\_path = GOOGLE\_CREDENTIALS\_PATH

&nbsp;       self.token\_path = TOKEN\_PICKLE\_PATH

&nbsp;       self.service = None

&nbsp;       self.authenticated = False

&nbsp;       self.creds = None

&nbsp;

&nbsp;   def authenticate(self) -> Tuple\[bool, str]:

&nbsp;       """

&nbsp;       Authenticate with Google Drive API using OAuth 2.0

&nbsp;

&nbsp;       Returns:

&nbsp;           Tuple of (success: bool, message: str)

&nbsp;       """

&nbsp;       try:

&nbsp;           # Load existing token if available

&nbsp;           if self.token\_path.exists():

&nbsp;               with open(self.token\_path, 'rb') as token:

&nbsp;                   self.creds = pickle.load(token)

&nbsp;

&nbsp;           # If no valid credentials, perform OAuth flow

&nbsp;           if not self.creds or not self.creds.valid:

&nbsp;               if self.creds and self.creds.expired and self.creds.refresh\_token:

&nbsp;                   # Refresh expired token

&nbsp;                   self.creds.refresh(Request())

&nbsp;               else:

&nbsp;                   # Check if credentials file exists

&nbsp;                   if not self.credentials\_path.exists():

&nbsp;                       return False, (

&nbsp;                           "‚ùå Google credentials file not found!\\n\\n"

&nbsp;                           "\*\*Setup Instructions:\*\*\\n"

&nbsp;                           "1. Go to <https://console.cloud.google.com/\\n>"

&nbsp;                           "2. Create a project and enable Google Drive API\\n"

&nbsp;                           "3. Create OAuth 2.0 credentials (Desktop app)\\n"

&nbsp;                           "4. Download JSON and save as `credentials/google\_credentials.json`"

&nbsp;                       )

&nbsp;

&nbsp;                   try:

&nbsp;                       # Perform OAuth flow

&nbsp;                       flow = InstalledAppFlow.from\_client\_secrets\_file(

&nbsp;                           str(self.credentials\_path),

&nbsp;                           GDRIVE\_SCOPES

&nbsp;                       )

&nbsp;                       self.creds = flow.run\_local\_server(port=0)

&nbsp;                   except Exception as e:

&nbsp;                       return False, f"‚ùå Authentication failed: {str(e)}"

&nbsp;

&nbsp;               # Save credentials for future runs

&nbsp;               self.token\_path.parent.mkdir(parents=True, exist\_ok=True)

&nbsp;               with open(self.token\_path, 'wb') as token:

&nbsp;                   pickle.dump(self.creds, token)

&nbsp;

&nbsp;           # Build Drive service

&nbsp;           try:

&nbsp;               self.service = build('drive', 'v3', credentials=self.creds)

&nbsp;               self.authenticated = True

&nbsp;               return True, "‚úÖ Successfully authenticated with Google Drive"

&nbsp;           except Exception as e:

&nbsp;               return False, f"‚ùå Failed to build service: {str(e)}"

&nbsp;

&nbsp;       except Exception as e:

&nbsp;           return False, f"‚ùå Unexpected error during authentication: {str(e)}"

&nbsp;

&nbsp;   def get\_folder\_id(self, folder\_name: str) -> Optional\[str]:

&nbsp;       """

&nbsp;       Get folder ID from folder name

&nbsp;

&nbsp;       Args:

&nbsp;           folder\_name: Name of the folder to find

&nbsp;

&nbsp;       Returns:

&nbsp;           Folder ID if found, None otherwise

&nbsp;       """

&nbsp;       if not self.authenticated:

&nbsp;           return None

&nbsp;

&nbsp;       try:

&nbsp;           query = f"name='{folder\_name}' and mimeType='application/vnd.google-apps.folder' and trashed=false"

&nbsp;           results = self.service.files().list(

&nbsp;               q=query,

&nbsp;               spaces='drive',

&nbsp;               fields='files(id, name)',

&nbsp;               pageSize=10

&nbsp;           ).execute()

&nbsp;

&nbsp;           items = results.get('files', \[])

&nbsp;           if items:

&nbsp;               return items\[0]\['id']

&nbsp;           return None

&nbsp;

&nbsp;       except HttpError as error:

&nbsp;           st.error(f"Error finding folder '{folder\_name}': {error}")

&nbsp;           return None

&nbsp;

&nbsp;   def list\_files\_in\_folder(

&nbsp;       self,

&nbsp;       folder\_id: str,

&nbsp;       file\_extension: Optional\[str] = None

&nbsp;   ) -> List\[Dict]:

&nbsp;       """

&nbsp;       List all files in a folder, optionally filtered by extension

&nbsp;

&nbsp;       Args:

&nbsp;           folder\_id: Google Drive folder ID

&nbsp;           file\_extension: Optional file extension filter (e.g., '.opus')

&nbsp;

&nbsp;       Returns:

&nbsp;           List of file metadata dictionaries

&nbsp;       """

&nbsp;       if not self.authenticated:

&nbsp;           return \[]

&nbsp;

&nbsp;       try:

&nbsp;           query = f"'{folder\_id}' in parents and trashed=false"

&nbsp;           if file\_extension:

&nbsp;               query += f" and name contains '{file\_extension}'"

&nbsp;

&nbsp;           results = self.service.files().list(

&nbsp;               q=query,

&nbsp;               spaces='drive',

&nbsp;               fields='files(id, name, mimeType, createdTime, modifiedTime, size)',

&nbsp;               pageSize=1000

&nbsp;           ).execute()

&nbsp;

&nbsp;           return results.get('files', \[])

&nbsp;

&nbsp;       except HttpError as error:

&nbsp;           st.error(f"Error listing files: {error}")

&nbsp;           return \[]

&nbsp;

&nbsp;   def list\_subfolders(self, folder\_id: str) -> List\[Dict]:

&nbsp;       """

&nbsp;       List all subfolders within a folder

&nbsp;

&nbsp;       Args:

&nbsp;           folder\_id: Parent folder ID

&nbsp;

&nbsp;       Returns:

&nbsp;           List of subfolder metadata dictionaries

&nbsp;       """

&nbsp;       if not self.authenticated:

&nbsp;           return \[]

&nbsp;

&nbsp;       try:

&nbsp;           query = (

&nbsp;               f"'{folder\_id}' in parents and "

&nbsp;               "mimeType='application/vnd.google-apps.folder' and "

&nbsp;               "trashed=false"

&nbsp;           )

&nbsp;

&nbsp;           results = self.service.files().list(

&nbsp;               q=query,

&nbsp;               spaces='drive',

&nbsp;               fields='files(id, name, createdTime)',

&nbsp;               pageSize=1000

&nbsp;           ).execute()

&nbsp;

&nbsp;           return results.get('files', \[])

&nbsp;

&nbsp;       except HttpError as error:

&nbsp;           st.error(f"Error listing subfolders: {error}")

&nbsp;           return \[]

&nbsp;

&nbsp;   def download\_file(

&nbsp;       self,

&nbsp;       file\_id: str,

&nbsp;       destination\_path: Path,

&nbsp;       progress\_callback=None

&nbsp;   ) -> Tuple\[bool, str]:

&nbsp;       """

&nbsp;       Download a file from Google Drive

&nbsp;

&nbsp;       Args:

&nbsp;           file\_id: Google Drive file ID

&nbsp;           destination\_path: Local path to save file

&nbsp;           progress\_callback: Optional callback function for progress updates

&nbsp;

&nbsp;       Returns:

&nbsp;           Tuple of (success: bool, message: str)

&nbsp;       """

&nbsp;       if not self.authenticated:

&nbsp;           return False, "Not authenticated with Google Drive"

&nbsp;

&nbsp;       try:

&nbsp;           request = self.service.files().get\_media(fileId=file\_id)

&nbsp;

&nbsp;           # Ensure directory exists

&nbsp;           destination\_path.parent.mkdir(parents=True, exist\_ok=True)

&nbsp;

&nbsp;           # Download with progress tracking

&nbsp;           with io.FileIO(str(destination\_path), 'wb') as fh:

&nbsp;               downloader = MediaIoBaseDownload(fh, request)

&nbsp;               done = False

&nbsp;

&nbsp;               while not done:

&nbsp;                   status, done = downloader.next\_chunk()

&nbsp;                   if progress\_callback and status:

&nbsp;                       progress\_callback(int(status.progress() \* 100))

&nbsp;

&nbsp;           return True, str(destination\_path)

&nbsp;

&nbsp;       except HttpError as error:

           return False, f"Download failed: {error}"

       except Exception as e:
           return False, f"Unexpected error: {str(e)}"

   def download_text_files(
       self,
       text_folder_path: str = GDRIVE_TEXT_FOLDER,
       local_dir: Path = TEXT_DIR,
       progress_callback=None
   ) -> Tuple[List[Dict], str]:
       """
       Download all text files from Investigation_Text folder

       Args:
           text_folder_path: Google Drive folder name
           local_dir: Local directory to save files
           progress_callback: Optional Streamlit progress bar

       Returns:
           Tuple of (downloaded_files: List[Dict], message: str)
       """
       if not self.authenticated:
           return [], "Not authenticated with Google Drive"

       folder_id = self.get_folder_id(text_folder_path)
       if not folder_id:
           return [], f"Folder '{text_folder_path}' not found in Google Drive"

       files = self.list_files_in_folder(folder_id)
       downloaded_files = []

       for file in files:
           success, path = self.download_file(
               file['id'],
               local_dir / file['name'],
               progress_callback
           )
           if success:
               downloaded_files.append({
                   'name': file['name'],
                   'path': path,
                   'size': file.get('size', 0)
               })

       return downloaded_files, f"Downloaded {len(downloaded_files)} text files"

   def download_audio_files(
       self,
       audio_folder_path: str = GDRIVE_AUDIO_FOLDER,
       local_dir: Path = AUDIO_DIR,
       progress_callback=None
   ) -> Tuple[List[Dict], str]:
       """
       Download all audio files from Investigation_Audio folder

       Args:
           audio_folder_path: Google Drive folder name
           local_dir: Local directory to save files
           progress_callback: Optional Streamlit progress bar

       Returns:
           Tuple of (downloaded_files: List[Dict], message: str)
       """
       if not self.authenticated:
           return [], "Not authenticated with Google Drive"

       folder_id = self.get_folder_id(audio_folder_path)
       if not folder_id:
           return [], f"Folder '{audio_folder_path}' not found in Google Drive"

       files = self.list_files_in_folder(folder_id)
       downloaded_files = []

       for file in files:
           if any(file['name'].lower().endswith(ext) for ext in SUPPORTED_AUDIO_FORMATS):
               success, path = self.download_file(
                   file['id'],
                   local_dir / file['name'],
                   progress_callback
               )
               if success:
                   downloaded_files.append({
                       'name': file['name'],
                       'path': path,
                       'size': file.get('size', 0)
                   })

       return downloaded_files, f"Downloaded {len(downloaded_files)} audio files"

---

 PART 7: FORENSICS ENGINE (forensics.py)

"""
Audio Forensic Analysis Engine for Die Waarheid
Performs bio-signal detection and stress analysis
"""

import numpy as np
import librosa
import soundfile as sf
from pathlib import Path
from typing import Dict, Tuple, List
from dataclasses import dataclass
from config import (
    TARGET_SAMPLE_RATE,
    STRESS_THRESHOLD_HIGH,
    SILENCE_RATIO_THRESHOLD,
    INTENSITY_SPIKE_THRESHOLD
)

@dataclass
class ForensicMetrics:
    """Container for forensic analysis results"""
    pitch_volatility: float
    silence_ratio: float
    intensity_max: float
    intensity_mean: float
    mfcc_variance: float
    zero_crossing_rate: float
    spectral_centroid: float
    stress_indicator: str
    cognitive_load: str

class ForensicsEngine:
    """
    Advanced audio forensic analysis
    Detects stress indicators and bio-signals
    """

    def __init__(self):
        self.sample_rate = TARGET_SAMPLE_RATE

    def load_audio(self, file_path: Path) -> Tuple[np.ndarray, int]:
        """Load audio file with resampling"""
        try:
            y, sr = librosa.load(str(file_path), sr=self.sample_rate)
            return y, sr
        except Exception as e:
            raise ValueError(f"Failed to load audio: {str(e)}")

    def extract_pitch(self, y: np.ndarray, sr: int) -> np.ndarray:
        """Extract fundamental frequency using PYIN"""
        f0, voiced_flag, voiced_probs = librosa.pyin(
            y,
            fmin=librosa.note_to_hz('C2'),
            fmax=librosa.note_to_hz('C7'),
            sr=sr
        )
        return f0

    def calculate_pitch_volatility(self, f0: np.ndarray) -> float:
        """Calculate pitch variation as stress indicator"""
        valid_f0 = f0[~np.isnan(f0)]
        if len(valid_f0) < 2:
            return 0.0
        return float(np.std(np.diff(valid_f0)) / (np.mean(valid_f0) + 1e-6) * 100)

    def calculate_silence_ratio(self, y: np.ndarray, sr: int, threshold_db: float = -40) -> float:
        """Calculate proportion of silence in audio"""
        S = librosa.feature.melspectrogram(y=y, sr=sr)
        S_db = librosa.power_to_db(S, ref=np.max)
        silence_frames = np.sum(np.max(S_db, axis=0) < threshold_db)
        return float(silence_frames / S_db.shape[1])

    def calculate_intensity(self, y: np.ndarray) -> Tuple[float, float]:
        """Calculate RMS energy (intensity) metrics"""
        rms = librosa.feature.rms(y=y)[0]
        return float(np.max(rms)), float(np.mean(rms))

    def calculate_mfcc_variance(self, y: np.ndarray, sr: int) -> float:
        """Calculate MFCC variance for voice quality"""
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        return float(np.var(mfcc))

    def calculate_zero_crossing_rate(self, y: np.ndarray) -> float:
        """Calculate zero crossing rate"""
        zcr = librosa.feature.zero_crossing_rate(y)[0]
        return float(np.mean(zcr))

    def calculate_spectral_centroid(self, y: np.ndarray, sr: int) -> float:
        """Calculate spectral centroid"""
        centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
        return float(np.mean(centroid))

    def analyze(self, file_path: Path) -> ForensicMetrics:
        """Perform complete forensic analysis"""
        y, sr = self.load_audio(file_path)

        f0 = self.extract_pitch(y, sr)
        pitch_volatility = self.calculate_pitch_volatility(f0)
        silence_ratio = self.calculate_silence_ratio(y, sr)
        intensity_max, intensity_mean = self.calculate_intensity(y)
        mfcc_variance = self.calculate_mfcc_variance(y, sr)
        zcr = self.calculate_zero_crossing_rate(y)
        spectral_centroid = self.calculate_spectral_centroid(y, sr)

        stress_indicator = "HIGH" if pitch_volatility > STRESS_THRESHOLD_HIGH else "NORMAL"
        cognitive_load = "HIGH" if silence_ratio > SILENCE_RATIO_THRESHOLD else "NORMAL"

        return ForensicMetrics(
            pitch_volatility=pitch_volatility,
            silence_ratio=silence_ratio,
            intensity_max=intensity_max,
            intensity_mean=intensity_mean,
            mfcc_variance=mfcc_variance,
            zero_crossing_rate=zcr,
            spectral_centroid=spectral_centroid,
            stress_indicator=stress_indicator,
            cognitive_load=cognitive_load
        )

---

 OPTIMIZATION RECOMMENDATIONS

## 1. PERFORMANCE OPTIMIZATION

### Audio Processing
- **Enable Numba JIT Compilation**: Uncomment `numba==0.58.1` in requirements.txt for 10-50x speedup on librosa operations
- **Batch Processing**: Process multiple audio files in parallel using `concurrent.futures.ThreadPoolExecutor` (already configured with MAX_WORKERS=4)
- **Audio Caching**: Cache processed audio features in SQLite to avoid re-processing identical files
- **Model Quantization**: Use quantized Whisper models (tiny/base) for faster transcription with acceptable accuracy trade-off

### Memory Management
- **Streaming Processing**: For large audio files (>100MB), use librosa's chunk-based processing instead of loading entire file
- **Garbage Collection**: Explicitly call `gc.collect()` after processing large batches
- **Memory Pooling**: Reuse numpy arrays instead of creating new ones in loops

### API Optimization
- **Request Batching**: Batch multiple Gemini API calls into single requests where possible
- **Response Caching**: Cache Gemini analysis results for identical transcripts (24-hour TTL)
- **Rate Limiting**: Implement exponential backoff for API rate limits (429 responses)

## 2. ACCURACY IMPROVEMENTS

### Audio Analysis
- **Multi-Model Ensemble**: Run Whisper on multiple model sizes and average confidence scores
- **Language-Specific Tuning**: Afrikaans-specific phoneme detection for better stress analysis
- **Noise Reduction**: Pre-process audio with spectral subtraction before analysis
- **Speaker Normalization**: Normalize pitch metrics per speaker baseline instead of global thresholds

### Text Analysis
- **Context Windows**: Increase GEMINI_MAX_TOKENS to 16384 for better context understanding
- **Few-Shot Prompting**: Add example analyses to system prompts for consistency
- **Contradiction Detection**: Implement semantic similarity scoring (cosine distance) for statement comparison
- **Temporal Analysis**: Track statement changes over time with version control

## 3. SCALABILITY IMPROVEMENTS

### Database Layer
- **Add SQLite Backend**: Store all analysis results for quick retrieval and reporting
- **Indexing Strategy**: Index by timestamp, speaker, and forensic flags for fast queries
- **Incremental Processing**: Only process new files since last run (track via database)

### Distributed Processing
- **Celery Integration**: Move long-running tasks (transcription, analysis) to background queue
- **Redis Caching**: Cache API responses and intermediate results
- **Horizontal Scaling**: Design for multi-worker deployment on cloud platforms

### Storage Optimization
- **Compression**: Store audio in FLAC format (lossless, 40% smaller than WAV)
- **Tiered Storage**: Archive old analysis results to cheaper storage after 90 days
- **Deduplication**: Detect and skip duplicate audio files using SHA-256 hashing

## 4. SECURITY HARDENING

### Credential Management
- **Secrets Rotation**: Implement automatic API key rotation every 90 days
- **Encrypted Storage**: Use cryptography library to encrypt sensitive data at rest
- **Audit Logging**: Log all API calls and data access with timestamps and user IDs

### Data Protection
- **Field-Level Encryption**: Encrypt PII (names, phone numbers) in database
- **Secure Deletion**: Overwrite temp files with random data before deletion
- **Access Control**: Implement role-based access control (RBAC) for different user types

## 5. RELIABILITY & MONITORING

### Error Handling
- **Retry Logic**: Implement exponential backoff for transient failures (network, API timeouts)
- **Circuit Breaker**: Disable failing services gracefully (e.g., if Gemini API is down)
- **Dead Letter Queue**: Store failed processing jobs for manual review

### Monitoring & Observability
- **Structured Logging**: Use JSON logging for easy parsing and analysis
- **Metrics Collection**: Track processing time, accuracy, API costs using Prometheus
- **Health Checks**: Implement `/health` endpoint for deployment monitoring
- **Error Alerting**: Send alerts for critical failures (API auth, storage full, etc.)

## 6. UI/UX IMPROVEMENTS

### Streamlit Enhancements
- **Progress Indicators**: Add detailed progress bars for long operations
- **Result Caching**: Use @st.cache_data for expensive computations
- **Dark Mode Toggle**: Add theme switcher for accessibility
- **Export Options**: Support PDF, HTML, JSON exports with custom templates

### Dashboard Features
- **Real-Time Updates**: Use WebSocket for live analysis progress
- **Interactive Visualizations**: Add drill-down capability to charts
- **Comparison Mode**: Side-by-side analysis of multiple conversations
- **Custom Filters**: Allow filtering by date range, speaker, stress level

## 7. DEPLOYMENT OPTIMIZATION

### Docker Containerization
```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
EXPOSE 8501
CMD ["streamlit", "run", "app.py"]
```

### Cloud Deployment
- **AWS Lambda**: Serverless transcription using AWS Transcribe (cheaper than Whisper)
- **Cloud Storage**: Use S3/GCS instead of local filesystem for scalability
- **CDN**: Serve static assets (visualizations, exports) via CloudFront/Cloud CDN
- **Auto-Scaling**: Configure horizontal pod autoscaling based on queue depth

## 8. COST OPTIMIZATION

### API Usage
- **Model Selection**: Use gemini-1.5-flash instead of gemini-1.5-pro for 50% cost reduction
- **Batch Processing**: Process files in batches to reduce API overhead
- **Local Fallbacks**: Use open-source models (Ollama, LLaMA) for non-critical analysis
- **Usage Monitoring**: Track API costs per analysis type and optimize high-cost operations

### Infrastructure
- **Spot Instances**: Use AWS Spot/GCP Preemptible instances for batch processing (70% savings)
- **Reserved Capacity**: Reserve compute for baseline load
- **Right-Sizing**: Monitor resource utilization and adjust instance types

## 9. TESTING & QUALITY ASSURANCE

### Test Coverage
- **Unit Tests**: Test each forensic metric calculation independently
- **Integration Tests**: Test end-to-end pipeline with sample data
- **Regression Tests**: Maintain baseline results for accuracy comparison
- **Load Tests**: Simulate 100+ concurrent users to identify bottlenecks

### Quality Metrics
- **Transcription Accuracy**: Compare Whisper output against manual transcripts (WER metric)
- **Stress Detection Validation**: Validate against known stress indicators
- **Report Consistency**: Ensure identical inputs produce identical outputs

## 10. DOCUMENTATION & MAINTENANCE

### Code Documentation
- **API Documentation**: Generate OpenAPI/Swagger docs for all endpoints
- **Architecture Diagrams**: Create C4 diagrams showing system components
- **Runbooks**: Document common troubleshooting procedures
- **Change Log**: Maintain detailed version history with breaking changes

### Operational Procedures
- **Backup Strategy**: Daily backups with 30-day retention
- **Disaster Recovery**: Document RTO/RPO targets and recovery procedures
- **Capacity Planning**: Monitor growth trends and plan infrastructure upgrades
- **Security Patches**: Establish SLA for applying security updates (24-48 hours)

---

## QUICK START CHECKLIST

- [ ] Install dependencies: `pip install -r requirements.txt`
- [ ] Create `.env` file with API keys
- [ ] Download Google credentials JSON to `credentials/`
- [ ] Run initial setup: `python -c "from config import validate_config; print(validate_config())"`
- [ ] Start application: `streamlit run app.py`
- [ ] Upload investigation files via Google Drive or manual upload
- [ ] Review generated reports in `data/output/reports/`

---

## SUPPORT & TROUBLESHOOTING

**Issue**: Whisper transcription is slow
- **Solution**: Switch to smaller model (`WHISPER_MODEL_SIZE = "base"`) or enable Numba

**Issue**: Out of memory during audio processing
- **Solution**: Reduce `BATCH_SIZE` or enable streaming mode for large files

**Issue**: Google Drive authentication fails
- **Solution**: Regenerate credentials at https://console.cloud.google.com and update `credentials/google_credentials.json`

**Issue**: Gemini API rate limiting
- **Solution**: Implement caching and reduce `GEMINI_MAX_TOKENS` or use gemini-1.5-flash

---

**Document Version**: 1.0.0
**Last Updated**: 2025-01-29
**Status**: PRODUCTION-READY
